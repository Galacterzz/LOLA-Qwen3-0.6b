{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxhnIv3m5Im8",
        "outputId": "9caef3c3-f4e4-46e3-f796-6749cf4875ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mTorch Version: 2.9.1+cu128\n",
            "CUDA Available: True\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U torch transformers peft datasets bitsandbytes trl accelerate\n",
        "!pip install -q -U scipy pandas scikit-learn\n",
        "\n",
        "import torch\n",
        "print(f\"Torch Version: {torch.__version__}\")\n",
        "print(f\"CUDA Available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455,
          "referenced_widgets": [
            "4b17633cd5b54343a1a6277d5c58e251",
            "e2f8b2bfeffb49afb6361b2a946cee15",
            "319f96fa373f4c45ae39918c6d75cc5b",
            "826e576f143d44348ec61d39bf85b8cd",
            "1e9bfa48036a482bbb7df018d2b59d7f",
            "41331ac6731c47edbc582516be8ac8a0",
            "ce45b8101174407d85cf7b277c981cd7",
            "5525318ed49c493fb21d4ad8cc96e8f0",
            "70f202971f2c4df8a59760ff58c591f8",
            "146b9158179b416d9cc8a6be1dcee7ba",
            "3449335ee28940fcb76ad1e9d17dcc83",
            "e1f59b4a8372425a868c86bfe924eb24",
            "12c195bbac26490c83cc65b5f987d2a9",
            "4aab005f242c4b7c9583e91ed3eebaef",
            "1b92f8cd3b1749678a870d8ea85ccecf",
            "913e6594322c464489d0d9a0cfb77cd3",
            "dfd96dbcd0a447c68907722e2318a9cc",
            "62a0dacb1e714286a96ed05fb0dc4b35",
            "9200fd15f13a4824937d24b4abb044c5",
            "ae5ecd171541481ea5409b3d1246f963",
            "4f68fa8fcbbc4e15b928898147810139",
            "6727412d646b4a3a83369c4cc16f50e1",
            "002dd6776a2d4a3e9ff385b49a2ca7d0",
            "74428c2de1c742be96c76fcded3a1c2e",
            "faf8665a0b7940e8b7ef4023440e90ae",
            "365d3b0d70264589a6040d76ef2cf774",
            "e5b19691789648859fcb724e25d2162d",
            "9f0e205b5ee94670acf4b33dd38604ed",
            "2dfa1316ca1d428e9cc66f3a4b0680bd",
            "ec79475c0bb14de5bbea91c71cb7d4aa",
            "55f1e0539db442c69442b7791f18e4dd",
            "b1ee9ee9b8614724bb2a985673ba9241",
            "e4da30b8919f4baeb3a797f56c1d120a",
            "51b9ebaca99d4e3c83ab96c18eb23c30",
            "1b6dc5b2522c4e49a07383dee7e777dc",
            "d6cabe131f2c4965b60f9d9b45e73373",
            "1345ac5918db4076b508d0e17362eb2d",
            "c51666817bc14feeb6105906349b5864",
            "704c2c5db221423c8086f93a8c425534",
            "64fd62af9c7d4f6b8d273e875e45aa1b",
            "aca808bcc3ff40a0aa05b52e8eac6c5e",
            "bf6ae153212947acb3455ec710701ba1",
            "42ee4b2669094012886a34dd9aea71e5",
            "1e9e3572f56049da9c4eb3d46f2ed1ed",
            "0dda308598bc4669bc522bef4995e07a",
            "215e68160f30463e9d9236a2f2a77e84",
            "18cebe9f3fd9485bae10a8b9ebbabbbd",
            "444544cab0e642c8870d338cef5583f6",
            "315a326d69374f3bb0a903b61baa8ad8",
            "314c32d7707440dcb3ae6cb9ccc8d782",
            "d228c96455f14c3ba76d8aafc5d3936f",
            "87da926a8f1744c689835b7ea13e41e1",
            "44c80f6f78ea4eedadf9664d9446c6aa",
            "d70a11574bb1468fa6c554221407e636",
            "7e5ed60c5f11489280bdf054823bdb7c",
            "42417dc3995e43ac855ab8420ac794b3",
            "2eff71ba1fdb49e9b081983d018ae88b",
            "1cf815e7d704474bb6325fa5e69f2b4f",
            "35697ad72de740568ee3af64c00120df",
            "9f6c1ec94b9043c08bb9521d8c2ff732",
            "d9cfac67cbfd433e9f4ea759dcbbc7df",
            "35bd0b51fcd742d287d4f6d12b087ed3",
            "2d8c7c777a404e15bb92a20e2dd64f28",
            "c510cd1c91f040eaa71d2ef609662302",
            "76201695a69e478abee20fa6eaa16b83",
            "fc0d3edcbe4f48b4a3515a456cb1ab09"
          ]
        },
        "id": "owAz7yZi5QpF",
        "outputId": "2120a733-d5ca-4dd3-c207-85ba90c4ccab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4b17633cd5b54343a1a6277d5c58e251",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1f59b4a8372425a868c86bfe924eb24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "002dd6776a2d4a3e9ff385b49a2ca7d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51b9ebaca99d4e3c83ab96c18eb23c30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dda308598bc4669bc522bef4995e07a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12376 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42417dc3995e43ac855ab8420ac794b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1654 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Samples: 12376\n",
            "Sample Input:\n",
            "<|im_start|>system\n",
            "You are an editor tasked with choosing the catchier one from several drafted headlines for the same article. Catchier means the one that is likely to generate more clicks.<|im_end|>\n",
            "<|im_start|>user\n",
            "You are presented with several headlines. Which one is catchier? **Return only the number before the headline. **No explanation is needed. No need to return the headline, only the number.****\n",
            "1. New York's Last Chance To Preserve Its Water Supply\n",
            "2. How YOU Can Help New York Stay U...\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_ID = \"Qwen/Qwen3-0.6B-Base\" # Using Qwen 2.5 (Stable SOTA small model)\n",
        "MAX_SEQ_LENGTH = 1024\n",
        "\n",
        "# Load Tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\" # Important for training\n",
        "\n",
        "# 1. Load Raw JSON\n",
        "def load_json(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "train_data = load_json(\"/content/drive/MyDrive/LOLA/Code files/kaggle output/finetune_train.json\")\n",
        "val_data = load_json(\"/content/drive/MyDrive/LOLA/Code files/kaggle output/finetune_calibrate.json\")\n",
        "test_data = load_json(\"/content/drive/MyDrive/LOLA/Code files/kaggle output/finetune_test.json\")\n",
        "\n",
        "# 2. Formatting Function (ChatML style)\n",
        "def apply_chat_template(example):\n",
        "    messages = example[\"messages\"]\n",
        "    formatted_text = \"\"\n",
        "    for message in messages:\n",
        "        role = message[\"role\"]\n",
        "        content = message[\"content\"]\n",
        "        formatted_text += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
        "\n",
        "    # For training, we add the generation prompt implies the assistant should speak next\n",
        "    # But since your data includes the assistant response in the JSON, the loop above covers it.\n",
        "    # The model learns to predict the tokens after <|im_start|>assistant\n",
        "    return {\"text\": formatted_text}\n",
        "\n",
        "# 3. Create Hugging Face Datasets\n",
        "train_dataset = Dataset.from_list(train_data).map(apply_chat_template)\n",
        "val_dataset = Dataset.from_list(val_data).map(apply_chat_template)\n",
        "# Note: Test dataset will be processed differently during inference\n",
        "\n",
        "print(f\"Train Samples: {len(train_dataset)}\")\n",
        "print(f\"Sample Input:\\n{train_dataset[0]['text'][:500]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "37f140c6b4bd461697a5f073b35c0335",
            "381343dd4d764a1b9b83c46eb4a08521",
            "10750a587aed4eb5b598d1e9a951bb26",
            "2ed53d818ee04a35adea4807d37670ea",
            "185222745e8c46b5989123afb7a4b887",
            "b478f32193a848cd8f3fe318b06494d2",
            "c1615b30bea94087b74cdb2c7c5e1b51",
            "2886af8cd7284464bfa9fc3ae8b16366",
            "f6dd333862bf4c37998fa6a221bace2f",
            "a632c4a3eba441b0a519b01ad8a289f6",
            "9bbdf0718ee94ddb8ef61d20db5c3214",
            "94e3e757a75e42e9ba84f84a0f63b763",
            "fb66741210df4d64b4ab3f5d26e0b835",
            "4c27bbecf0e24d9eba816e6ca87de8e0",
            "5091643a323a4ca5b66faaf9f2266092",
            "c17d822c349b4195a9a77e9a8d3c28b5",
            "8d109d5166574ce8b71116e17f42bb55",
            "e4b0fcf28da140c792109e66c8f6f9e9",
            "5a9ed943c92b4c618386812a3df15169",
            "a7d84a61ae4e4964bad64d0d3883a3f4",
            "3725f60bd438479fad33495202984a50",
            "77a62421dcf245d98f227d3eb48218b7",
            "a6d28a78cb54464d9ae4611c31b752f0",
            "3ee43bf8c62e41839efd45ab56afec6a",
            "c846a5713379446786bc6018285a53a3",
            "146fcf2b05b441429df01d8d3fcba074",
            "91c6993e415e456e934de11992491683",
            "0df37d948f87457e8a964f141a6e563f",
            "e456dbf9778847bea3cd724eb7ce1e38",
            "cef4394719e0448bb67390744794ddb3",
            "43c5cb5d1d6842d4bc6a335b3c088f2b",
            "43539f4399324dd4823d27fde1b510cc",
            "1b93edd4845c432a802e5ca158e5306f"
          ]
        },
        "id": "-HQP_PNs6lMS",
        "outputId": "ed132064-a437-46ed-f71c-5c20cf15f871"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37f140c6b4bd461697a5f073b35c0335",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94e3e757a75e42e9ba84f84a0f63b763",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6d28a78cb54464d9ae4611c31b752f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 10,092,544 || all params: 606,142,464 || trainable%: 1.6650\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "\n",
        "# 1. Quantization Config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# 2. Load Base Model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Prepare for k-bit training (freezes weights, casts layer norm to float32)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# 3. LoRA Configuration\n",
        "peft_config = LoraConfig(\n",
        "    r=16,                    # Rank: Higher = more parameters to train (16 is standard)\n",
        "    lora_alpha=32,           # Scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477,
          "referenced_widgets": [
            "2597670c331e425abd1f4078d3333a36",
            "7bb4f2603cc647fabf013dc0af6d93fb",
            "c56949d1a99b4924b6451307d97813a2",
            "8dc745bcddd640df970c423409eb8362",
            "5c71e11994e041ad867ca795dfe39436",
            "b3a84505d51a45848a784e65ef3209da",
            "e5ce077553244f8ca1d511c6cd1b7ce7",
            "79c6211222f14c62a8fef4be0cf51d70",
            "a3bfc65f4e1348a6966e1d981282ded2",
            "bbed4754aa2845ce94fdad0ad5143ba4",
            "a1f676cd421a409194244709120294d8",
            "e509085628e941f293d1d48f20714d5e",
            "7886de9985e549a89dea7e3d0628101d",
            "479cf5c2df9b4b57a7a0c147ffdf5a2b",
            "42502e185b944df8bbb7b2057a00793d",
            "53806d28fb3447ffaa2c080ea20f2de0",
            "08d8f48401b94637998e8b9b34e6c7f9",
            "40be7508669b4f82a9e95a29bc55e330",
            "e066936d90824ece922dbd0247132169",
            "060d4f01e409466a8310027d683c3dbb",
            "df48abab7c7d445296f884d43fd5dd6b",
            "d618e688f19343c6a2faa0fbb8539904",
            "a10ad64c580344ac937d3dcb5783f187",
            "7b0dd7cfc8bc4d369c90a1662fabb230",
            "9e559032607e4d4ea8b5ad34aa5bf49a",
            "8bdea69e33544c80a7a18958a5a85e26",
            "0ab0927ac52b4d27a563567eebbbb1b7",
            "a6ea3578197b4e7ca2be3243676faefa",
            "08e85129220c4b1cb0b59da2dc530dcd",
            "1f8003ff0cff48aa932b1d4cb3973316",
            "165c0544be0844388ba36a4b99ab5803",
            "045f08c5556d4af897773dad022c09e9",
            "21ebd412a2c6487a9686b0271ff7699c",
            "701145c2f5a24f1db78ed4e9075b1b08",
            "9a7039099f88496ca280b4bf6b6db4f8",
            "23d135a701c44bd5a3d2e5636b7ea525",
            "a1e2a20309c04c5f919e9fe08349edd2",
            "3ecf55ecccf548528cbe6484eef3a073",
            "8d3d213240404ba6b6a0f59b7e2c225c",
            "d01228c1038147bc8683d65d25587f3c",
            "03b75d7efcfa4c80a0cfea138081f5c9",
            "93c7455c05e94609a898dfb34be5473c",
            "24a7f8a5c6f3495ab4e961823dd6dab2",
            "a92ffd22e2c44edbb45313b85536af52"
          ]
        },
        "id": "ySzcdriy7Ge7",
        "outputId": "637a4d1e-1ec8-44ac-8a92-81e6b0dd4ecd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2597670c331e425abd1f4078d3333a36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/12376 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e509085628e941f293d1d48f20714d5e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/12376 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a10ad64c580344ac937d3dcb5783f187",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/1654 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "701145c2f5a24f1db78ed4e9075b1b08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/1654 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 07:32, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Entropy</th>\n",
              "      <th>Num Tokens</th>\n",
              "      <th>Mean Token Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.598900</td>\n",
              "      <td>1.565850</td>\n",
              "      <td>1.605086</td>\n",
              "      <td>150746.000000</td>\n",
              "      <td>0.720856</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.555700</td>\n",
              "      <td>1.548326</td>\n",
              "      <td>1.570260</td>\n",
              "      <td>300178.000000</td>\n",
              "      <td>0.722957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to final_adapter_checkpoint\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "OUTPUT_DIR = \"qwen_finetuned_results\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=10,\n",
        "    max_steps=100, # Short run for demonstration. For real results, try 300-500 or 1 epoch.\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    args=training_args\n",
        ")\n",
        "\n",
        "print(\"Starting Training...\")\n",
        "trainer.train()\n",
        "\n",
        "# Save the best adapter\n",
        "ADAPTER_PATH = \"final_adapter_checkpoint\"\n",
        "trainer.model.save_pretrained(ADAPTER_PATH)\n",
        "tokenizer.save_pretrained(ADAPTER_PATH)\n",
        "print(f\"Model saved to {ADAPTER_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lNRCsg_7YcZ",
        "outputId": "80705f0a-7f1d-42a7-af33-a6ce225d2366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating predictions on Test Set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3263/3263 [39:10<00:00,  1.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions to 'predicted_best_headlines.csv'\n",
            "   test_id  predicted_option  \\\n",
            "0    14444                 1   \n",
            "1    14514                 1   \n",
            "2    14691                 1   \n",
            "3    15003                 1   \n",
            "4    15019                 1   \n",
            "\n",
            "                                       best_headline  \n",
            "0  A Reading Of 'Dinner With Monoliths' By Joseph...  \n",
            "1  A Music Video With All Of My Favorite People, ...  \n",
            "2  A Father Wrote His Kid 14 Things To Always Rem...  \n",
            "3  There’s Nothing Funny About Ferguson. But This...  \n",
            "4  This Is Not Your Typical 'Inspiration Porn' St...  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from peft import PeftModel\n",
        "\n",
        "# 1. Reload Base Model & Adapter for Inference\n",
        "# We reload to ensure clean state\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "model.eval()\n",
        "\n",
        "# 2. Load Test CSV for IDs\n",
        "df_test_csv = pd.read_csv(\"/content/drive/MyDrive/LOLA/Code files/kaggle output/final_test.csv\")\n",
        "# We need a way to map the sequential JSON items to the CSV groups\n",
        "# Assuming the 'finetune_test.json' was generated in the same order as unique 'test_id' groups in CSV\n",
        "unique_test_ids = df_test_csv['test_id'].unique()\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"Generating predictions on Test Set...\")\n",
        "\n",
        "for i, item in enumerate(tqdm(test_data)):\n",
        "    # Prepare Prompt: Remove the assistant's answer from the JSON messages if it exists (it shouldn't in true test, but just in case)\n",
        "    messages = [m for m in item[\"messages\"] if m[\"role\"] != \"assistant\"]\n",
        "\n",
        "    # Construct input text\n",
        "    input_text = \"\"\n",
        "    for message in messages:\n",
        "        input_text += f\"<|im_start|>{message['role']}\\n{message['content']}<|im_end|>\\n\"\n",
        "    input_text += \"<|im_start|>assistant\\n\" # Prompt for the answer\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=5,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False # Greedy decoding for deterministic best option\n",
        "        )\n",
        "\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the response part (everything after the prompt)\n",
        "    # Since we don't have the full raw string easily separable, we just look at the newly generated tokens\n",
        "    # But 'decode' gives the whole string. Let's just regex search the last digit.\n",
        "\n",
        "    # Simple heuristic: Find the number following \"assistant\"\n",
        "    try:\n",
        "        # Split by the prompt end if possible, or just search the whole string for the LAST number\n",
        "        response_only = generated_text[len(input_text):] # This is rough if special tokens are skipped differently\n",
        "\n",
        "        # Better: look for single digit 1-9\n",
        "        match = re.search(r'\\b([1-9])\\b', generated_text.split(\"assistant\")[-1])\n",
        "        predicted_option = int(match.group(1)) if match else 1 # Default to 1 if parse fails\n",
        "    except:\n",
        "        predicted_option = 1\n",
        "\n",
        "    # Get Test ID\n",
        "    current_test_id = unique_test_ids[i] if i < len(unique_test_ids) else \"Unknown\"\n",
        "\n",
        "    results.append({\n",
        "        \"test_id\": current_test_id,\n",
        "        \"predicted_option\": predicted_option\n",
        "    })\n",
        "\n",
        "# 3. Create Final CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Optional: Join with original CSV to get the actual headline text for that option\n",
        "# This is complex because we need to lookup (test_id, option_number) -> headline\n",
        "merged_data = []\n",
        "for idx, row in results_df.iterrows():\n",
        "    tid = row['test_id']\n",
        "    opt = row['predicted_option']\n",
        "\n",
        "    # Find row in original csv\n",
        "    match = df_test_csv[(df_test_csv['test_id'] == tid) & (df_test_csv['option_number'] == opt)]\n",
        "\n",
        "    if not match.empty:\n",
        "        headline_text = match.iloc[0]['headline']\n",
        "    else:\n",
        "        headline_text = \"Headline not found\"\n",
        "\n",
        "    merged_data.append({\n",
        "        \"test_id\": tid,\n",
        "        \"predicted_option\": opt,\n",
        "        \"best_headline\": headline_text\n",
        "    })\n",
        "\n",
        "final_df = pd.DataFrame(merged_data)\n",
        "final_df.to_csv(\"predicted_best_headlines.csv\", index=False)\n",
        "print(\"Saved predictions to 'predicted_best_headlines.csv'\")\n",
        "print(final_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY05slSoAaAL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
