{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13848141,"sourceType":"datasetVersion","datasetId":8820751}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T11:28:18.991117Z","iopub.execute_input":"2025-11-24T11:28:18.991358Z","iopub.status.idle":"2025-11-24T11:28:19.004049Z","shell.execute_reply.started":"2025-11-24T11:28:18.991323Z","shell.execute_reply":"2025-11-24T11:28:19.003277Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/qwen-lola-fine-tune/train.csv\n/kaggle/input/qwen-lola-fine-tune/test.csv\n/kaggle/input/qwen-lola-fine-tune/calibrate.csv\n/kaggle/input/qwen-lola-fine-tune/filtered-ctr-all.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport os\n\n# --- Configuration for train data---\nINPUT_FILE = '/kaggle/input/qwen-lola-fine-tune/train.csv'  # Your input CSV file\nOUTPUT_JSON = 'finetune_train.json'    # Output file for LLM training\nOUTPUT_CSV = 'final_train.csv' # Output CSV with tracking columns\n\nSYSTEM_PROMPT = \"You are an editor tasked with choosing the catchier one from several drafted headlines for the same article. Catchier means the one that is likely to generate more clicks.\"\nUSER_INSTRUCTION = \"You are presented with several headlines. Which one is catchier? **Return only the number before the headline. **No explanation is needed. No need to return the headline, only the number.****\\n\"\n\ndef prepare_data():\n    # 1. Load the Data\n    if not os.path.exists(INPUT_FILE):\n        print(f\"Error: {INPUT_FILE} not found. Please place your CSV file in the same folder.\")\n        # Creating a dummy file for demonstration purposes if you run this without your file\n        print(\"Creating a dummy CSV for demonstration...\")\n        data = {\n            'test_id': [14366, 14366, 14366, 14332, 14332, 14332],\n            'headline': [\n                'What They Found', 'A Science Mystery', 'He Sat There', \n                'The Selfies', 'Warning: Graphic', 'Sigh... I Want One'\n            ],\n            'CTR': [0.011, 0.012, 0.005, 0.041, 0.033, 0.035]\n        }\n        df = pd.DataFrame(data)\n    else:\n        df = pd.read_csv(INPUT_FILE)\n\n    # Clean data: Ensure headlines are strings and handle NaNs if any\n    df['headline'] = df['headline'].astype(str)\n    \n    # 2. Prepare storage for results\n    json_dataset = []\n    \n    # We will add columns to the original dataframe to track assignments\n    df['option_number'] = 0\n    df['is_best'] = 0\n\n    # 3. Group by test_id and process\n    grouped = df.groupby('test_id')\n\n    print(f\"Found {len(grouped)} unique test groups. Processing...\")\n\n    for test_id, group in grouped:\n        headlines = group['headline'].tolist()\n        ctrs = group['CTR'].tolist()\n        indices = group.index.tolist() # Keep track of original dataframe indices\n        \n        # Identify the winner (Index of max CTR in this specific group)\n        # We use .index() on the list to find the local position (0, 1, 2...)\n        max_ctr = max(ctrs)\n        best_local_index = ctrs.index(max_ctr)\n        \n        # Calculate the \"Human Readable\" option number (1-based)\n        best_option_number = best_local_index + 1\n\n        # Construct User Message\n        user_message = USER_INSTRUCTION\n        for i, headline in enumerate(headlines):\n            option_num = i + 1\n            user_message += f\"{option_num}. {headline}\\n\"\n            \n            # Update the original DataFrame with tracking info\n            original_idx = indices[i]\n            df.at[original_idx, 'option_number'] = option_num\n            if i == best_local_index:\n                df.at[original_idx, 'is_best'] = 1\n\n        # Construct JSON Entry\n        recording = {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_message},\n                {\"role\": \"assistant\", \"content\": str(best_option_number)}\n            ]\n        }\n        json_dataset.append(recording)\n\n    # 4. Save Outputs\n    \n    # Save JSON for Unsloth/Fine-tuning\n    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n        json.dump(json_dataset, f, indent=4)\n    \n    # Save processed CSV for your reference\n    df.to_csv(OUTPUT_CSV, index=False)\n\n    print(f\"Success! Processed {len(df)} rows.\")\n    print(f\"1. Training data saved to: {OUTPUT_JSON}\")\n    print(f\"2. Tracking CSV saved to: {OUTPUT_CSV}\")\n    print(\"\\nSample JSON Entry:\")\n    print(json.dumps(json_dataset[0], indent=2))\n\nif __name__ == \"__main__\":\n    prepare_data()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration for calibrate data---\nINPUT_FILE = '/kaggle/input/qwen-lola-fine-tune/calibrate.csv'  # Your input CSV file\nOUTPUT_JSON = 'finetune_calibrate.json'    # Output file for LLM training\nOUTPUT_CSV = 'final_calibrate.csv' # Output CSV with tracking columns\n\nSYSTEM_PROMPT = \"You are an editor tasked with choosing the catchier one from several drafted headlines for the same article. Catchier means the one that is likely to generate more clicks.\"\nUSER_INSTRUCTION = \"You are presented with several headlines. Which one is catchier? **Return only the number before the headline. **No explanation is needed. No need to return the headline, only the number.****\\n\"\n\ndef prepare_data():\n    # 1. Load the Data\n    if not os.path.exists(INPUT_FILE):\n        print(f\"Error: {INPUT_FILE} not found. Please place your CSV file in the same folder.\")\n        # Creating a dummy file for demonstration purposes if you run this without your file\n        print(\"Creating a dummy CSV for demonstration...\")\n        data = {\n            'test_id': [14366, 14366, 14366, 14332, 14332, 14332],\n            'headline': [\n                'What They Found', 'A Science Mystery', 'He Sat There', \n                'The Selfies', 'Warning: Graphic', 'Sigh... I Want One'\n            ],\n            'CTR': [0.011, 0.012, 0.005, 0.041, 0.033, 0.035]\n        }\n        df = pd.DataFrame(data)\n    else:\n        df = pd.read_csv(INPUT_FILE)\n\n    # Clean data: Ensure headlines are strings and handle NaNs if any\n    df['headline'] = df['headline'].astype(str)\n    \n    # 2. Prepare storage for results\n    json_dataset = []\n    \n    # We will add columns to the original dataframe to track assignments\n    df['option_number'] = 0\n    df['is_best'] = 0\n\n    # 3. Group by test_id and process\n    grouped = df.groupby('test_id')\n\n    print(f\"Found {len(grouped)} unique test groups. Processing...\")\n\n    for test_id, group in grouped:\n        headlines = group['headline'].tolist()\n        ctrs = group['CTR'].tolist()\n        indices = group.index.tolist() # Keep track of original dataframe indices\n        \n        # Identify the winner (Index of max CTR in this specific group)\n        # We use .index() on the list to find the local position (0, 1, 2...)\n        max_ctr = max(ctrs)\n        best_local_index = ctrs.index(max_ctr)\n        \n        # Calculate the \"Human Readable\" option number (1-based)\n        best_option_number = best_local_index + 1\n\n        # Construct User Message\n        user_message = USER_INSTRUCTION\n        for i, headline in enumerate(headlines):\n            option_num = i + 1\n            user_message += f\"{option_num}. {headline}\\n\"\n            \n            # Update the original DataFrame with tracking info\n            original_idx = indices[i]\n            df.at[original_idx, 'option_number'] = option_num\n            if i == best_local_index:\n                df.at[original_idx, 'is_best'] = 1\n\n        # Construct JSON Entry\n        recording = {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_message},\n                {\"role\": \"assistant\", \"content\": str(best_option_number)}\n            ]\n        }\n        json_dataset.append(recording)\n\n    # 4. Save Outputs\n    \n    # Save JSON for Unsloth/Fine-tuning\n    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n        json.dump(json_dataset, f, indent=4)\n    \n    # Save processed CSV for your reference\n    df.to_csv(OUTPUT_CSV, index=False)\n\n    print(f\"Success! Processed {len(df)} rows.\")\n    print(f\"1. Training data saved to: {OUTPUT_JSON}\")\n    print(f\"2. Tracking CSV saved to: {OUTPUT_CSV}\")\n    print(\"\\nSample JSON Entry:\")\n    print(json.dumps(json_dataset[0], indent=2))\n\nif __name__ == \"__main__\":\n    prepare_data()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration for calibrate data---\nINPUT_FILE = '/kaggle/input/qwen-lola-fine-tune/test.csv'  # Your input CSV file\nOUTPUT_JSON = 'finetune_test.json'    # Output file for LLM training\nOUTPUT_CSV = 'final_test.csv' # Output CSV with tracking columns\n\nSYSTEM_PROMPT = \"You are an editor tasked with choosing the catchier one from several drafted headlines for the same article. Catchier means the one that is likely to generate more clicks.\"\nUSER_INSTRUCTION = \"You are presented with several headlines. Which one is catchier? **Return only the number before the headline. **No explanation is needed. No need to return the headline, only the number.****\\n\"\n\ndef prepare_data():\n    # 1. Load the Data\n    if not os.path.exists(INPUT_FILE):\n        print(f\"Error: {INPUT_FILE} not found. Please place your CSV file in the same folder.\")\n        # Creating a dummy file for demonstration purposes if you run this without your file\n        print(\"Creating a dummy CSV for demonstration...\")\n        data = {\n            'test_id': [14366, 14366, 14366, 14332, 14332, 14332],\n            'headline': [\n                'What They Found', 'A Science Mystery', 'He Sat There', \n                'The Selfies', 'Warning: Graphic', 'Sigh... I Want One'\n            ],\n            'CTR': [0.011, 0.012, 0.005, 0.041, 0.033, 0.035]\n        }\n        df = pd.DataFrame(data)\n    else:\n        df = pd.read_csv(INPUT_FILE)\n\n    # Clean data: Ensure headlines are strings and handle NaNs if any\n    df['headline'] = df['headline'].astype(str)\n    \n    # 2. Prepare storage for results\n    json_dataset = []\n    \n    # We will add columns to the original dataframe to track assignments\n    df['option_number'] = 0\n    df['is_best'] = 0\n\n    # 3. Group by test_id and process\n    grouped = df.groupby('test_id')\n\n    print(f\"Found {len(grouped)} unique test groups. Processing...\")\n\n    for test_id, group in grouped:\n        headlines = group['headline'].tolist()\n        ctrs = group['CTR'].tolist()\n        indices = group.index.tolist() # Keep track of original dataframe indices\n        \n        # Identify the winner (Index of max CTR in this specific group)\n        # We use .index() on the list to find the local position (0, 1, 2...)\n        max_ctr = max(ctrs)\n        best_local_index = ctrs.index(max_ctr)\n        \n        # Calculate the \"Human Readable\" option number (1-based)\n        best_option_number = best_local_index + 1\n\n        # Construct User Message\n        user_message = USER_INSTRUCTION\n        for i, headline in enumerate(headlines):\n            option_num = i + 1\n            user_message += f\"{option_num}. {headline}\\n\"\n            \n            # Update the original DataFrame with tracking info\n            original_idx = indices[i]\n            df.at[original_idx, 'option_number'] = option_num\n            if i == best_local_index:\n                df.at[original_idx, 'is_best'] = 1\n\n        # Construct JSON Entry\n        recording = {\n            \"messages\": [\n                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": user_message},\n                {\"role\": \"assistant\", \"content\": str(best_option_number)}\n            ]\n        }\n        json_dataset.append(recording)\n\n    # 4. Save Outputs\n    \n    # Save JSON for Unsloth/Fine-tuning\n    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:\n        json.dump(json_dataset, f, indent=4)\n    \n    # Save processed CSV for your reference\n    df.to_csv(OUTPUT_CSV, index=False)\n\n    print(f\"Success! Processed {len(df)} rows.\")\n    print(f\"1. Training data saved to: {OUTPUT_JSON}\")\n    print(f\"2. Tracking CSV saved to: {OUTPUT_CSV}\")\n    print(\"\\nSample JSON Entry:\")\n    print(json.dumps(json_dataset[0], indent=2))\n\nif __name__ == \"__main__\":\n    prepare_data()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch datasets transformers peft bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}