{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9097149,"sourceType":"datasetVersion","datasetId":5467449},{"sourceId":13730172,"sourceType":"datasetVersion","datasetId":8735585}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T17:34:26.571953Z","iopub.execute_input":"2025-11-23T17:34:26.572233Z","iopub.status.idle":"2025-11-23T17:34:26.591232Z","shell.execute_reply.started":"2025-11-23T17:34:26.572212Z","shell.execute_reply":"2025-11-23T17:34:26.590530Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/qwen-embedding/qwen_embeddings.csv\n/kaggle/input/qwen-embedding/qwen_embeddings (1).npy\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/upworthy-archive-confirmatory-packages-03.12.2020.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/LoRA_CTR_test.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/upworthy-archive-exploratory-packages-03.12.2020.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/winner-all.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/selected_pairs_df_005_256.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/LoRA_CTR_train.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/selected_pairs_df_005_3072.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/upworthy-archive-holdout-packages-03.12.2020.csv\n/kaggle/input/lola-llm-assisted-online-learning-algorithm/all_test_headline_embed_3072.csv\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# 1. Data Description:\n   This is the Headline A/B testing dataset of Upworthy. The data is of the A/B test conducted in between year 2013-2015. The dataset has Headlines and their impressions and clicks they got. We would assess the headline through their click-through-ratio(CTR).","metadata":{}},{"cell_type":"code","source":"#Combine three subsets to create a complete dataset\ncsv_file_path_1 = '/kaggle/input/lola-llm-assisted-online-learning-algorithm/upworthy-archive-exploratory-packages-03.12.2020.csv'\ncsv_file_path_2 = '/kaggle/input/lola-llm-assisted-online-learning-algorithm/upworthy-archive-confirmatory-packages-03.12.2020.csv'\ncsv_file_path_3 = '/kaggle/input/lola-llm-assisted-online-learning-algorithm/upworthy-archive-holdout-packages-03.12.2020.csv'\ncsv_file_path_4 = '/kaggle/input/lola-llm-assisted-online-learning-algorithm/LoRA_CTR_train.csv'\ncsv_file_path_5 = '/kaggle/input/lola-llm-assisted-online-learning-algorithm/LoRA_CTR_test.csv'\n\n# Read the files, converting column 15 to string type\ndf1 = pd.read_csv(csv_file_path_1, dtype={15: str})\ndf2 = pd.read_csv(csv_file_path_2, dtype={15: str})\ndf3 = pd.read_csv(csv_file_path_3, dtype={15: str})\nctr_train = pd.read_csv(csv_file_path_4, dtype={15: str})\nctr_test = pd.read_csv(csv_file_path_5, dtype={15: str})\n\ndf = pd.concat([df1, df2, df3])\ndf.to_csv('upworthy-archive-packages-all.csv', index=False) # saves the combined dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:33:34.129046Z","iopub.execute_input":"2025-11-23T16:33:34.129378Z","iopub.status.idle":"2025-11-23T16:33:38.714348Z","shell.execute_reply.started":"2025-11-23T16:33:34.129355Z","shell.execute_reply":"2025-11-23T16:33:38.713522Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# 2. Data Statistics\n- Packages: A individual headline \n- Tests: Each test had multiple packages compared with each other\n- Clicks: Number of people who clicked the article\n- Impression: Number of people who viewed the headline\n- clickabality_id: id for each test conducted\n- CTR: It is the ratio of Clicks/Total Impression\n","metadata":{}},{"cell_type":"code","source":"# print(f\"---------- first few Rows ----------\")\n# print(df.head(10))\n# print('\\n \\n \\n \\n \\n')\n\nprint(f'---------- Datatype of each Column ----------')\nprint(df.info())\nprint('\\n')\n\nprint(f'---------- Dataset info ----------')\n\nprint('\\n')\n# Dataset shape info\nprint(f'shape of our datset: {df.shape}')\n\nprint('\\n')\n# packages information\nnum_of_packages = len(df['clickability_test_id'])\nprint(f'Total number of packages: {num_of_packages}')\n\n# tests information\nnum_of_tests = len(df['clickability_test_id'].unique())\nprint(f'number of tests: {num_of_tests}')\n\nprint('\\n')\n# user engagement info\ntotal_impressions = df['impressions'].sum()\ntotal_clicks = df['clicks'].sum()\nprint(f'Total number of impressions in complete dataset: {total_impressions}')\nprint(f'Total number of clicks in the complete datset: {total_clicks}')\n# df[['impressions', 'clicks']].sum()\n\n\n# general info\naverage_packages_per_test = num_of_packages / num_of_tests\naverage_clicks = total_clicks / num_of_packages\naverage_impressions = total_impressions / num_of_packages \naverage_ctr = total_clicks / total_impressions\n\nprint('\\n')\nprint(f'average packages(Headlines) per tests: {average_packages_per_test:.2f}')\nprint(f'average clicks per package(Headline): {average_clicks:.2f}')\nprint(f'average impressions per packages(Headlines): {average_impressions:.2f}')\nprint(f'average ctr per package(Headline): {average_ctr:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:33:42.855648Z","iopub.execute_input":"2025-11-23T16:33:42.855945Z","iopub.status.idle":"2025-11-23T16:33:42.961106Z","shell.execute_reply.started":"2025-11-23T16:33:42.855924Z","shell.execute_reply":"2025-11-23T16:33:42.960326Z"}},"outputs":[{"name":"stdout","text":"---------- Datatype of each Column ----------\n<class 'pandas.core.frame.DataFrame'>\nIndex: 150817 entries, 0 to 22599\nData columns (total 17 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   Unnamed: 0            150817 non-null  int64  \n 1   created_at            150817 non-null  object \n 2   updated_at            150817 non-null  object \n 3   clickability_test_id  150817 non-null  object \n 4   excerpt               134790 non-null  object \n 5   headline              150816 non-null  object \n 6   lede                  150713 non-null  object \n 7   slug                  150817 non-null  object \n 8   eyecatcher_id         150636 non-null  object \n 9   impressions           150817 non-null  int64  \n 10  clicks                150817 non-null  int64  \n 11  significance          150817 non-null  float64\n 12  first_place           150817 non-null  bool   \n 13  winner                150817 non-null  bool   \n 14  share_text            20900 non-null   object \n 15  square                49800 non-null   object \n 16  test_week             150817 non-null  int64  \ndtypes: bool(2), float64(1), int64(4), object(10)\nmemory usage: 18.7+ MB\nNone\n\n\n---------- Dataset info ----------\n\n\nshape of our datset: (150817, 17)\n\n\nTotal number of packages: 150817\nnumber of tests: 32487\n\n\nTotal number of impressions in complete dataset: 538272878\nTotal number of clicks in the complete datset: 8182674\n\n\naverage packages(Headlines) per tests: 4.64\naverage clicks per package(Headline): 54.26\naverage impressions per packages(Headlines): 3569.05\naverage ctr per package(Headline): 0.02\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# 3. Data Filtering","metadata":{}},{"cell_type":"code","source":"# convert the int64 values to FP64\ndf['clicks'] = df['clicks'].astype(float) \ndf['impressions'] = df['impressions'].astype(float) \n\n# Create a CTR column\ndf['CTR'] = df['clicks']/df['impressions']\n\n# convert tto datetime format dtype\ndf['created_at'] = pd.to_datetime(df['created_at'], format='mixed', errors='coerce')\n\n# Only takes tests which has more than 1 package, this step takes time\nfiltered_groups = df.groupby(['clickability_test_id', 'eyecatcher_id']).filter(lambda x: x['headline'].nunique() > 1).reset_index(drop=True)\n\n# Drop unnecessary columns\nnew_df = filtered_groups[['clickability_test_id', 'eyecatcher_id', 'created_at','headline', 'CTR','clicks', 'impressions']].drop_duplicates()\n\n# saves this filtered dataset\nnew_df.to_csv('filtered-ctr-all.csv', index=False)\n\ndel df, new_df # removes the data from current memory ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:33:47.101647Z","iopub.execute_input":"2025-11-23T16:33:47.102216Z","iopub.status.idle":"2025-11-23T16:33:55.647810Z","shell.execute_reply.started":"2025-11-23T16:33:47.102190Z","shell.execute_reply":"2025-11-23T16:33:55.647155Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/working/filtered-ctr-all.csv')\ndf = df.sort_values(by='created_at').reset_index(drop=True)\ndf['new_id'] = df.groupby(['clickability_test_id', 'eyecatcher_id']).ngroup()\n\n# chek few values to understand the structure\n# df.head(30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:34:18.708924Z","iopub.execute_input":"2025-11-23T16:34:18.709714Z","iopub.status.idle":"2025-11-23T16:34:19.036991Z","shell.execute_reply.started":"2025-11-23T16:34:18.709691Z","shell.execute_reply":"2025-11-23T16:34:19.036364Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# print(f\"---------- first few Rows ----------\")\n# print(df.head(10))\n# print('\\n \\n \\n \\n \\n')\n\nprint(f'---------- Datatype of each Column ----------')\nprint(df.info())\nprint('\\n')\n\nprint(f'---------- Dataset info ----------')\n\nprint('\\n')\n# Dataset shape info\nprint(f'shape of our datset: {df.shape}')\n\nprint('\\n')\n# packages information\nfiltered_num_of_packages = len(df['clickability_test_id'])\nprint(f'Total number of packages: {filtered_num_of_packages}')\n\n# tests information\nfiltered_num_of_tests = len(df['new_id'].unique())\nprint(f'number of tests: {filtered_num_of_tests}')\n\nprint('\\n')\n# user engagement info\nfiltered_total_impressions = df['impressions'].sum()\nfiltered_total_clicks = df['clicks'].sum()\nprint(f'Total number of impressions in complete dataset: {filtered_total_impressions}')\nprint(f'Total number of clicks in the complete datset: {filtered_total_clicks}')\n# df[['impressions', 'clicks']].sum()\n\n\n# general info\nfiltered_average_packages_per_test = filtered_num_of_packages / filtered_num_of_tests\nfiltered_average_clicks = filtered_total_clicks / filtered_num_of_packages\nfiltered_average_impressions = filtered_total_impressions / filtered_num_of_packages \nfiltered_average_ctr = filtered_total_clicks / filtered_total_impressions\n\nprint('\\n')\nprint(f'average packages(Headlines) per tests: {filtered_average_packages_per_test:.2f}')\nprint(f'average clicks per package(Headline): {filtered_average_clicks:.2f}')\nprint(f'average impressions per packages(Headlines): {filtered_average_impressions:.2f}')\nprint(f'average ctr per package(Headline): {filtered_average_ctr:.2f}')\n\nprint('\\n')\n# Get the frequency of each new_id value\nid_counts = df['new_id'].value_counts().sort_index()\nfrequency_table = id_counts.value_counts().sort_index()\n\n# Set the threshold for grouping\nthreshold = 7\n\nprint(\"Frequency Table - Number of Rows with Same new_id\")\nprint(\"=\" * 65)\nprint(f\"{'Rows per new_id':<20} {'Number of Groups':<20} {'% of Dataset':<15}\")\nprint(\"-\" * 65)\n\n# Display counts below threshold\nfor num_rows in range(1, threshold):\n    if num_rows in frequency_table.index:\n        num_groups = frequency_table[num_rows]\n        percentage = (num_groups / filtered_num_of_tests) * 100\n        print(f\"{num_rows:<20} {num_groups:<20} {percentage:.2f}%\")\n\n# Display grouped count for threshold and above\ngrouped_groups = frequency_table[frequency_table.index >= threshold].sum()\nif grouped_groups > 0:\n    percentage_grouped = (grouped_groups / filtered_num_of_tests) * 100\n    print(f\"{f'{threshold} or more':<20} {grouped_groups:<20} {percentage_grouped:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:35:03.643100Z","iopub.execute_input":"2025-11-23T16:35:03.643641Z","iopub.status.idle":"2025-11-23T16:35:03.689078Z","shell.execute_reply.started":"2025-11-23T16:35:03.643618Z","shell.execute_reply":"2025-11-23T16:35:03.688480Z"}},"outputs":[{"name":"stdout","text":"---------- Datatype of each Column ----------\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 77245 entries, 0 to 77244\nData columns (total 8 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   clickability_test_id  77245 non-null  object \n 1   eyecatcher_id         77245 non-null  object \n 2   created_at            77245 non-null  object \n 3   headline              77244 non-null  object \n 4   CTR                   77245 non-null  float64\n 5   clicks                77245 non-null  float64\n 6   impressions           77245 non-null  float64\n 7   new_id                77245 non-null  int64  \ndtypes: float64(3), int64(1), object(4)\nmemory usage: 4.7+ MB\nNone\n\n\n---------- Dataset info ----------\n\n\nshape of our datset: (77245, 8)\n\n\nTotal number of packages: 77245\nnumber of tests: 17681\n\n\nTotal number of impressions in complete dataset: 277338713.0\nTotal number of clicks in the complete datset: 3741517.0\n\n\naverage packages(Headlines) per tests: 4.37\naverage clicks per package(Headline): 48.44\naverage impressions per packages(Headlines): 3590.38\naverage ctr per package(Headline): 0.01\n\n\nFrequency Table - Number of Rows with Same new_id\n=================================================================\nRows per new_id      Number of Groups     % of Dataset   \n-----------------------------------------------------------------\n2                    1619                 9.16%\n3                    939                  5.31%\n4                    8836                 49.97%\n5                    2964                 16.76%\n6                    2685                 15.19%\n7 or more            638                  3.61%\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 4. Train-Test Split","metadata":{}},{"cell_type":"code","source":"# Determine the indices for train and test split\ntrain_size = int(len(df) * 0.7)\ntest_size = int(len(df) * 0.2)\n\n# Create the train and test DataFrames\ntrain_df = df.iloc[:train_size]\ntest_df = df.iloc[-test_size:]\n\n# Display the resulting DataFrames\nprint(\"Train DataFrame (first 70%):\")\nprint(f'Shape of train data by time: {train_df.shape}')\n\nprint(\"\\nTest DataFrame (last 20%):\")\nprint(f'shape of test data by time: {test_df.shape}')\n\ntrain_df.to_csv('train_order_by_time.csv', index=False)\ntest_df.to_csv('test_order_by_time.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:38:31.548109Z","iopub.execute_input":"2025-11-23T16:38:31.548936Z","iopub.status.idle":"2025-11-23T16:38:32.148440Z","shell.execute_reply.started":"2025-11-23T16:38:31.548908Z","shell.execute_reply":"2025-11-23T16:38:32.147649Z"}},"outputs":[{"name":"stdout","text":"Train DataFrame (first 70%):\nShape of train data by time: (54071, 8)\n\nTest DataFrame (last 20%):\nshape of test data by time: (15449, 8)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.model_selection import GroupShuffleSplit\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv('/kaggle/working/filtered-ctr-all.csv')\n\ndf = df[['clickability_test_id', 'eyecatcher_id', 'headline', 'impressions', 'clicks']]\ndf.loc[:, 'CTR'] = df['clicks'] / df['impressions']\ndf['test_id'] = df.groupby(['clickability_test_id', 'eyecatcher_id']).ngroup()\n\ntrain_ratio = 0.7\ncalibrate_ratio = 0.1\ntest_ratio = 0.2\n\ngss = GroupShuffleSplit(n_splits=1, test_size=test_ratio+calibrate_ratio, random_state=42)\ntrain_idx, temp_idx = next(gss.split(df, groups=df['test_id']))\ntrain_df = df.iloc[train_idx]\ntemp_df = df.iloc[temp_idx]\n\ngss2 = GroupShuffleSplit(n_splits=1, test_size=test_ratio/(test_ratio+calibrate_ratio), random_state=42)\ncalibrate_idx, test_idx = next(gss2.split(temp_df, groups=temp_df['test_id']))\ncalibrate_df = temp_df.iloc[calibrate_idx]\ntest_df = temp_df.iloc[test_idx]\n\ndel temp_df\n\nprint(f'headlines in train date: {len(train_df)}')\nprint(f'headlines in test data: {len(test_df)}')\nprint(f'headlines in calibration data: {len(calibrate_df)}')\n\nunique_headlines_train = set(train_df['headline'].unique())\ntest_df = test_df[~test_df['headline'].isin(unique_headlines_train)]\ncalibrate_df = calibrate_df[~calibrate_df['headline'].isin(unique_headlines_train)]\n\nprint('\\nremoving the duplicate from the training data the test and calibrate data size reduces')\nprint(f'- removed data leak headlines: {len(test_df)}')\nprint(f'- removed data leak headlines: {len(calibrate_df)}') # this used as validation data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:42:23.469838Z","iopub.execute_input":"2025-11-23T16:42:23.470638Z","iopub.status.idle":"2025-11-23T16:42:23.825909Z","shell.execute_reply.started":"2025-11-23T16:42:23.470611Z","shell.execute_reply":"2025-11-23T16:42:23.825230Z"}},"outputs":[{"name":"stdout","text":"headlines in train date: 53965\nheadlines in test data: 15483\nheadlines in calibration data: 7797\n\nremoving the duplicate from the training data the test and calibrate data size reduces\n- removed data leak headlines: 12039\n- removed data leak headlines: 6072\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"train_df.to_csv('train.csv', index=False)\ntest_df.to_csv('test.csv', index=False)\ncalibrate_df.to_csv('calibrate.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:43:30.478120Z","iopub.execute_input":"2025-11-23T16:43:30.478898Z","iopub.status.idle":"2025-11-23T16:43:31.038343Z","shell.execute_reply.started":"2025-11-23T16:43:30.478867Z","shell.execute_reply":"2025-11-23T16:43:31.037594Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# 5. Embedding Creation","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport math\nfrom pathlib import Path\nfrom tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\n\nDEVICE = \"cuda\"\nprint(\"Device:\", DEVICE)\n\n# Replace this path with your actual CSV path if different\nDATA_PATH_TRAIN = \"/kaggle/working/train.csv\"  # <--- change if needed\nDATA_PATH_CALIBRATE = \"/kaggle/working/calibrate.csv\"\nDATA_PATH_TEST = \"/kaggle/working/test.csv\"\n\ndf_train = pd.read_csv(DATA_PATH_TRAIN)  # we have loaded the training data\ndf_calibrate = pd.read_csv(DATA_PATH_CALIBRATE)\ndf_test = pd.read_csv(DATA_PATH_TEST)\n\n# quick checks\nassert 'headline' in df_train.columns, \"CSV must contain a 'headline' column\"\nassert 'CTR' in df_train.columns, \"CSV must contain a 'CTR' column\"\nprint(\"the columns 'headline' and 'CTR' exists\")\n\nassert 'headline' in df_calibrate.columns, \"CSV must contain a 'headline' column\"\nassert 'CTR' in df_calibrate.columns, \"CSV must contain a 'CTR' column\"\nprint(\"the columns 'headline' and 'CTR' exists\")\n\nassert 'headline' in df_test.columns, \"CSV must contain a 'headline' column\"\nassert 'CTR' in df_test.columns, \"CSV must contain a 'CTR' column\"\nprint(\"the columns 'headline' and 'CTR' exists\")\n\n# Basic preprocessing function (customize as you like)\ndef preprocess_text(s):\n    if pd.isna(s):\n        return \"\"\n    # minimal preprocessing: strip, replace multiple spaces\n    t = str(s).strip()\n    t = \" \".join(t.split())\n    return t\n\n# Apply preprocessing and deduplicate headings optionally\ndf_train['headline'] = df_train['headline'].astype(str).apply(preprocess_text)\ndf_train = df_train.reset_index(drop=True)\n\ndf_calibrate['headline'] = df_calibrate['headline'].astype(str).apply(preprocess_text)\ndf_calibrate = df_calibrate.reset_index(drop=True)\n\ndf_test['headline'] = df_test['headline'].astype(str).apply(preprocess_text)\ndf_test = df_test.reset_index(drop=True)\n\nprint(f\"Loaded train {len(df_train)} headlines. Example:\\n\", df_train.head(3))\nprint(f\"Loaded calibrate{len(df_calibrate)} headlines. Example:\\n\", df_calibrate.head(3))\nprint(f\"Loaded test {len(df_test)} headlines. Example:\\n\", df_test.head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T16:59:46.641183Z","iopub.execute_input":"2025-11-23T16:59:46.641908Z","iopub.status.idle":"2025-11-23T16:59:46.953197Z","shell.execute_reply.started":"2025-11-23T16:59:46.641883Z","shell.execute_reply":"2025-11-23T16:59:46.952584Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nthe columns 'headline' and 'CTR' exists\nthe columns 'headline' and 'CTR' exists\nthe columns 'headline' and 'CTR' exists\nLoaded train 53965 headlines. Example:\n        clickability_test_id             eyecatcher_id  \\\n0  546e009a9ad54ec65b00004b  546c7f2dbadeb5788700000a   \n1  546e009a9ad54ec65b00004b  546c7f2dbadeb5788700000a   \n2  546e009a9ad54ec65b00004b  546c7f2dbadeb5788700000a   \n\n                                            headline  impressions  clicks  \\\n0  What They Learned From The Scientist Was Terri...       4594.0    51.0   \n1  A Science Guy Helps 3 Dudes From America Under...       4571.0    58.0   \n2  He Sat Them Down And Told Them About An Immine...       4601.0    27.0   \n\n        CTR  test_id  \n0  0.011101    14366  \n1  0.012689    14366  \n2  0.005868    14366  \nLoaded calibrate6072 headlines. Example:\n        clickability_test_id             eyecatcher_id  \\\n0  546f889587942aedcb000048  546ebd5b92f391daa3000014   \n1  546f889587942aedcb000048  546ebd5b92f391daa3000014   \n2  546f889587942aedcb000048  546ebd5b92f391daa3000014   \n\n                                            headline  impressions  clicks  \\\n0  I Thought They Were Just Sharing Their Passion...       3024.0    43.0   \n1  When These Bright-Eyed Young People Shared The...       3018.0    33.0   \n2  When These Seemingly Okay Individuals Shared T...       3061.0    35.0   \n\n        CTR  test_id  \n0  0.014220    14420  \n1  0.010934    14420  \n2  0.011434    14420  \nLoaded test 12039 headlines. Example:\n        clickability_test_id             eyecatcher_id  \\\n0  546ff5410e78bdd64600000f  546ff54b0e78bdd646000016   \n1  546ff5410e78bdd64600000f  546ff54b0e78bdd646000016   \n2  54737147d289f68ac400001a  545c03114601954a15000009   \n\n                                            headline  impressions  clicks  \\\n0  A Reading Of 'Dinner With Monoliths' By Joseph...       3078.0     6.0   \n1  How The First Dinner I Had With A Boyfriend An...       3069.0    19.0   \n2  A Music Video With All Of My Favorite People, ...       3011.0    20.0   \n\n        CTR  test_id  \n0  0.001949    14444  \n1  0.006191    14444  \n2  0.006642    14514  \n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport torch\n\n# qwen_model_name = \"Qwen/Qwen3-Embedding-8B\"\nqwen_model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n\n\nprint(f\"Trying SentenceTransformer({qwen_model_name}) ...\")\nqwen_model = SentenceTransformer(qwen_model_name)\nprint(f\"Succesfully loaded SentenceTransformer({qwen_model_name}) ...\")\n\nbatch_size = 64\n\n\nprint('Creating embedding for training data')\n## ----- Train Embedding -----\n# creating all the embeddings\ntexts = df_train['headline'].tolist()\nall_embeddings = []\ndone = 0\nfor i in range(0, len(texts), batch_size):  \n    batch_texts = texts[i:i+batch_size]\n    if done >= 10000:\n        print(f'Embedings done: {done}')\n        done = 0\n    # print(f'Embedings done: {done}')\n    emb = qwen_model.encode(batch_texts)\n    # ensure numpy\n    all_embeddings.append(np.array(emb))\n    done += batch_size\nqwen_embeddings_train = np.vstack(all_embeddings)\n\n# saving the embeddings and csv with embedding\nqwen_path_npy = \"qwen_embeddings_train.npy\"\nnp.save(qwen_path_npy, qwen_embeddings_train)\nprint(f\"Saved embeddings numpy to: {qwen_path_npy}\")\n\nqwen_path_csv = \"qwen_embeddings_train.csv\"\ndf_out = pd.DataFrame({'headline': df_train['headline'], \n                       'CTR': df_train['CTR'], \n                       'embedding': [json.dumps(e.tolist()) for e in qwen_embeddings_train]})\ndf_out.to_csv(qwen_path_csv, index=False)\nprint(f\"Saved CSV to: {qwen_path_csv}\")\n\n\nprint('Creating embedding for calibration data')\n## ----- Calibrate Embedding -----\n# creating all the embeddings\ntexts = df_calibrate['headline'].tolist()\nall_embeddings = []\ndone = 0\nfor i in range(0, len(texts), batch_size):  \n    batch_texts = texts[i:i+batch_size]\n    if done >= 1000:\n        print(f'Embedings done: {done}')\n        done = 0\n    emb = qwen_model.encode(batch_texts)\n    # ensure numpy\n    all_embeddings.append(np.array(emb))\n    done += batch_size\nqwen_embeddings_calibrate = np.vstack(all_embeddings)\n\n# saving the embeddings and csv with embedding\nqwen_path_npy = \"qwen_embeddings_calibrate.npy\"\nnp.save(qwen_path_npy, qwen_embeddings_calibrate)\nprint(f\"Saved embeddings numpy to: {qwen_path_npy}\")\n\nqwen_path_csv = \"qwen_embeddings_calibrate.csv\"\ndf_out = pd.DataFrame({'headline': df_calibrate['headline'], \n                       'CTR': df_calibrate['CTR'], \n                       'embedding': [json.dumps(e.tolist()) for e in qwen_embeddings_calibrate]})\ndf_out.to_csv(qwen_path_csv, index=False)\nprint(f\"Saved CSV to: {qwen_path_csv}\")\n\n\nprint('Creating embedding for test data')\n## ----- Test Embedding -----\n# creating all the embeddings\ntexts = df_test['headline'].tolist()\nall_embeddings = []\ndone = 0\nfor i in range(0, len(texts), batch_size):  \n    batch_texts = texts[i:i+batch_size]\n    if done >= 1000:\n        print(f'Embedings done: {done}')\n        done = 0\n    emb = qwen_model.encode(batch_texts)\n    # ensure numpy\n    all_embeddings.append(np.array(emb))\n    done += batch_size\nqwen_embeddings_test = np.vstack(all_embeddings)\n\n# saving the embeddings and csv with embedding\nqwen_path_npy = \"qwen_embeddings_test.npy\"\nnp.save(qwen_path_npy, qwen_embeddings_test)\nprint(f\"Saved embeddings numpy to: {qwen_path_npy}\")\n\nqwen_path_csv = \"qwen_embeddings_test.csv\"\ndf_out = pd.DataFrame({'headline': df_test['headline'], \n                       'CTR': df_test['CTR'], \n                       'embedding': [json.dumps(e.tolist()) for e in qwen_embeddings_test]})\ndf_out.to_csv(qwen_path_csv, index=False)\nprint(f\"Saved CSV to: {qwen_path_csv}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T17:50:14.953328Z","iopub.execute_input":"2025-11-23T17:50:14.954087Z","iopub.status.idle":"2025-11-23T17:57:34.316653Z","shell.execute_reply.started":"2025-11-23T17:50:14.954060Z","shell.execute_reply":"2025-11-23T17:57:34.315944Z"}},"outputs":[{"name":"stdout","text":"Trying SentenceTransformer(Qwen/Qwen3-Embedding-0.6B) ...\nSuccesfully loaded SentenceTransformer(Qwen/Qwen3-Embedding-0.6B) ...\nCreating embedding for training data\nEmbedings done: 10048\nEmbedings done: 10048\nEmbedings done: 10048\nEmbedings done: 10048\nEmbedings done: 10048\nSaved embeddings numpy to: qwen_embeddings_train.npy\nSaved CSV to: qwen_embeddings_train.csv\nCreating embedding for calibration data\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nSaved embeddings numpy to: qwen_embeddings_calibrate.npy\nSaved CSV to: qwen_embeddings_calibrate.csv\nCreating embedding for test data\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nEmbedings done: 1024\nSaved embeddings numpy to: qwen_embeddings_test.npy\nSaved CSV to: qwen_embeddings_test.csv\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport json\nimport numpy as np\nimport pandas as pd\nimport random\nimport joblib\nimport math\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\nimport matplotlib.pyplot as plt\n\n# Paths\n# OUT_DIR = Path(\"outputs\")\n# EMB_DIR = \"/kaggle/input/qwen-embedding\"\nMODEL_DIR = Path(\"models\")\nTRAIN_DIR = Path(\"training\")\nTRAIN_DIR.mkdir(parents=True, exist_ok=True)\nMODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T17:59:57.148278Z","iopub.execute_input":"2025-11-23T17:59:57.148687Z","iopub.status.idle":"2025-11-23T17:59:57.159876Z","shell.execute_reply.started":"2025-11-23T17:59:57.148661Z","shell.execute_reply":"2025-11-23T17:59:57.159322Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"import numpy as np\ntrain_data = np.load('/kaggle/working/qwen_embeddings_train.npy')\nprint(train_data)\nprint(train_data.shape)\n\ncalibrate_data = np.load('/kaggle/working/qwen_embeddings_calibrate.npy')\nprint(calibrate_data)\nprint(calibrate_data.shape)\n\ntest_data = np.load('/kaggle/working/qwen_embeddings_test.npy')\nprint(test_data)\nprint(test_data.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:04:03.261280Z","iopub.execute_input":"2025-11-23T18:04:03.261598Z","iopub.status.idle":"2025-11-23T18:04:03.374698Z","shell.execute_reply.started":"2025-11-23T18:04:03.261577Z","shell.execute_reply":"2025-11-23T18:04:03.373927Z"}},"outputs":[{"name":"stdout","text":"[[-0.01979227  0.02034266 -0.00955267 ... -0.0189545   0.05996537\n  -0.02946962]\n [ 0.02401167 -0.03049346 -0.01257041 ... -0.01376737  0.01509776\n  -0.00046995]\n [-0.02812944 -0.00499259 -0.01171883 ...  0.04234135 -0.03107574\n  -0.01004842]\n ...\n [ 0.00937955 -0.01251978 -0.01221728 ...  0.0040628   0.02104761\n  -0.00253818]\n [-0.00240453 -0.02713586 -0.01258758 ...  0.00677357  0.00387503\n  -0.01317829]\n [-0.00472617 -0.02527185 -0.01252851 ...  0.01094262  0.01490286\n  -0.01516217]]\n(53965, 1024)\n[[ 0.00559677 -0.05313345 -0.00863181 ... -0.03989751  0.0113431\n  -0.01605429]\n [ 0.02003668 -0.02326474 -0.01197334 ...  0.02004261  0.01724499\n  -0.01138762]\n [-0.0063905  -0.04787905 -0.0114645  ... -0.01090019 -0.00906689\n  -0.01790828]\n ...\n [-0.08983362  0.05888341 -0.00955247 ... -0.01174458 -0.05532799\n  -0.01934313]\n [-0.08517613  0.03511509 -0.01249848 ... -0.02933976  0.00455961\n  -0.03781397]\n [-0.06771341  0.06507223 -0.01150156 ... -0.00324242 -0.02727953\n  -0.02608895]]\n(6072, 1024)\n[[ 0.00285824 -0.01577928 -0.01373298 ...  0.02391818  0.01376426\n  -0.01177556]\n [-0.0058003  -0.04236574 -0.01210347 ... -0.00120879 -0.00139567\n  -0.06532576]\n [ 0.01966118 -0.03045582 -0.0110987  ... -0.00952911 -0.00374712\n  -0.02758119]\n ...\n [-0.04435879  0.0122819  -0.00928608 ... -0.00506804 -0.0190737\n   0.00984342]\n [-0.01732593  0.05392455 -0.00803354 ... -0.02991782 -0.00471123\n  -0.01807445]\n [-0.03007454  0.0459311  -0.00722593 ... -0.03039521 -0.01089002\n   0.01296687]]\n(12039, 1024)\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"npy_path_train = \"/kaggle/working/qwen_embeddings_train.npy\"\ncsv_path_train = \"/kaggle/working/qwen_embeddings_train.csv\"\n\nnpy_path_calibrate = \"/kaggle/working/qwen_embeddings_calibrate.npy\"\ncsv_path_calibrate = \"/kaggle/working/qwen_embeddings_calibrate.csv\"\n\nnpy_path_test = \"/kaggle/working/qwen_embeddings_test.npy\"\ncsv_path_test = \"/kaggle/working/qwen_embeddings_test.csv\"\n\n# load the data\nX_train = np.load(npy_path_train) # npy file contain only embeddings, 1024 dimensional embedding vector\nX_valid = np.load(npy_path_calibrate)\nX_test = np.load(npy_path_test)\n\ndf_train = pd.read_csv(csv_path_train) # contains headline, CTR, embedding\ndf_calibrate = pd.read_csv(csv_path_calibrate)\ndf_test = pd.read_csv(csv_path_test)\n\n# headlines = df['headline'].astype(str).tolist()\n# y = df['CTR'].values.astype(float)\n\nprint(f'Shape of the input X: {X.shape}')\nprint(f'shape of the CTR output y: {y.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:10:41.744111Z","iopub.execute_input":"2025-11-23T18:10:41.744406Z","iopub.status.idle":"2025-11-23T18:10:54.792633Z","shell.execute_reply.started":"2025-11-23T18:10:41.744384Z","shell.execute_reply":"2025-11-23T18:10:54.791854Z"}},"outputs":[{"name":"stdout","text":"Shape of the input X: (53965, 1024)\nshape of the CTR output y: (53965,)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ---------------------------------------------------------\n# 1. MLP Class\n# ---------------------------------------------------------\nclass MLPRegressor(nn.Module):\n    def __init__(self, input_dim, hidden_dims=[512,256], dropout=0.2, activation=nn.ReLU):\n        super().__init__()\n        layers = []\n        prev = input_dim\n        for h in hidden_dims:\n            layers.append(nn.Linear(prev, h))\n            layers.append(activation())\n            layers.append(nn.Dropout(dropout))\n            prev = h\n        layers.append(nn.Linear(prev, 1))\n        self.net = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.net(x).squeeze(-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:28:11.843755Z","iopub.execute_input":"2025-11-23T18:28:11.844095Z","iopub.status.idle":"2025-11-23T18:28:11.850856Z","shell.execute_reply.started":"2025-11-23T18:28:11.844071Z","shell.execute_reply":"2025-11-23T18:28:11.849960Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# ---------------------------------------------------------\n# 2. Training Function (Modified to accept specific data)\n# ---------------------------------------------------------\ndef train_mlp_specific_data(X_train, y_train, X_val, y_val, \n                            headlines_val, # for saving predictions\n                            run_name=\"experiment\",\n                            hidden_dims=[512, 256],\n                            dropout=0.2,\n                            batch_size=64,\n                            lr=1e-3,\n                            weight_decay=1e-5,\n                            epochs=50,\n                            patience=6,\n                            checkpoint_every=5):\n    \n    print(f\"[{run_name}] Train shape: {X_train.shape}, Val shape: {X_val.shape}\")\n    \n    # Standard Scaling\n    # Fit ONLY on training data, transform both\n    x_scaler = StandardScaler().fit(X_train)\n    y_scaler = StandardScaler().fit(y_train.reshape(-1, 1))\n    \n    X_train_s = x_scaler.transform(X_train)\n    X_val_s = x_scaler.transform(X_val)\n    y_train_s = y_scaler.transform(y_train.reshape(-1, 1)).ravel()\n    y_val_s = y_scaler.transform(y_val.reshape(-1, 1)).ravel()\n    \n    # Create output directory for this run\n    run_dir = MODEL_DIR / run_name\n    run_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Datasets & Loaders\n    train_ds = TensorDataset(torch.from_numpy(X_train_s).float(), torch.from_numpy(y_train_s).float())\n    val_ds = TensorDataset(torch.from_numpy(X_val_s).float(), torch.from_numpy(y_val_s).float())\n    \n    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n    \n    # Model Setup\n    model = MLPRegressor(input_dim=X_train_s.shape[1], hidden_dims=hidden_dims, dropout=dropout).to(DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    criterion = nn.MSELoss()\n    \n    best_val_loss = float('inf')\n    epochs_no_improve = 0\n    history = []\n    \n    # Training Loop\n    \n    \n    for epoch in range(1, epochs+1):\n        model.train()\n        train_losses = []\n        for xb, yb in train_loader:\n            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n            pred = model(xb)\n            loss = criterion(pred, yb)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            train_losses.append(loss.item())\n        train_loss = np.mean(train_losses)\n        \n        # Validation\n        model.eval()\n        val_losses = []\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n                pred = model(xb)\n                loss = criterion(pred, yb)\n                val_losses.append(loss.item())\n        val_loss = np.mean(val_losses)\n        \n        history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss})\n        \n        # Checkpointing\n        if val_loss < best_val_loss - 1e-8:\n            best_val_loss = val_loss\n            epochs_no_improve = 0\n            torch.save(model.state_dict(), run_dir / \"best_model.pt\")\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"[{run_name}] Early stopping at epoch {epoch}\")\n                break\n                \n    # Save History and Metrics\n    hist_df = pd.DataFrame(history)\n    hist_df.to_csv(run_dir / \"history.csv\", index=False)\n    \n    # Load best model for final evaluation on validation set\n    model.load_state_dict(torch.load(run_dir / \"best_model.pt\", weights_only=True))\n    model.eval()\n    \n    with torch.no_grad():\n        val_X_tensor = torch.from_numpy(X_val_s).float().to(DEVICE)\n        pred_s = model(val_X_tensor).cpu().numpy()\n        \n    # Inverse transform to get original CTR scale\n    pred_orig = y_scaler.inverse_transform(pred_s.reshape(-1,1)).ravel()\n    \n    mse = mean_squared_error(y_val, pred_orig)\n    mae = mean_absolute_error(y_val, pred_orig)\n    r2 = r2_score(y_val, pred_orig)\n    \n    metrics = {\"mse\": mse, \"mae\": mae, \"r2\": r2, \"best_val_loss_scaled\": best_val_loss}\n    \n    return metrics, run_dir, x_scaler, y_scaler, model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:28:27.582039Z","iopub.execute_input":"2025-11-23T18:28:27.582665Z","iopub.status.idle":"2025-11-23T18:28:27.595299Z","shell.execute_reply.started":"2025-11-23T18:28:27.582640Z","shell.execute_reply":"2025-11-23T18:28:27.594334Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# ---------------------------------------------------------\n# 3. Main Execution\n# ---------------------------------------------------------\n\n# --- A. Load Data ---\nprint(\"Loading data...\")\nnpy_path_train = \"/kaggle/working/qwen_embeddings_train.npy\"\ncsv_path_train = \"/kaggle/working/qwen_embeddings_train.csv\"\n\nnpy_path_calibrate = \"/kaggle/working/qwen_embeddings_calibrate.npy\"\ncsv_path_calibrate = \"/kaggle/working/qwen_embeddings_calibrate.csv\"\n\nnpy_path_test = \"/kaggle/working/qwen_embeddings_test.npy\"\ncsv_path_test = \"/kaggle/working/qwen_embeddings_test.csv\"\n\nX_train_full = np.load(npy_path_train)\nX_calibrate = np.load(npy_path_calibrate)\nX_test = np.load(npy_path_test)\n\ndf_train = pd.read_csv(csv_path_train)\ndf_calibrate = pd.read_csv(csv_path_calibrate)\ndf_test = pd.read_csv(csv_path_test)\n\n# Extract Targets\ny_train_full = df_train['CTR'].values.astype(float)\ny_calibrate = df_calibrate['CTR'].values.astype(float)\ny_test = df_test['CTR'].values.astype(float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:29:14.665951Z","iopub.execute_input":"2025-11-23T18:29:14.666695Z","iopub.status.idle":"2025-11-23T18:29:28.007734Z","shell.execute_reply.started":"2025-11-23T18:29:14.666666Z","shell.execute_reply":"2025-11-23T18:29:28.007120Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# --- B. Hyperparameter Tuning (Small Subset) ---\nprint(\"\\n--- Starting Hyperparameter Tuning on Small Subset ---\")\n\n# Define Subset (e.g., first 20% of data for speed)\nsubset_size = int(len(X_train_full) * 0.2)\nX_train_sub = X_train_full[:subset_size]\ny_train_sub = y_train_full[:subset_size]\n\n# Hyperparameter Grid\nparam_grid = [\n    {\"hidden_dims\": [512, 256], \"lr\": 1e-3, \"dropout\": 0.2},\n    {\"hidden_dims\": [256, 128], \"lr\": 1e-3, \"dropout\": 0.1},\n    {\"hidden_dims\": [512, 256], \"lr\": 5e-4, \"dropout\": 0.2},\n]\n\nbest_params = None\nbest_score = float('inf') # Minimizing MSE\n\ntuning_results = []\n\nfor i, params in enumerate(param_grid):\n    run_name = f\"tune_run_{i}\"\n    print(f\"\\nTesting params: {params}\")\n    \n    # Validate on Calibrate set\n    metrics, _, _, _, _ = train_mlp_specific_data(\n        X_train_sub, y_train_sub, \n        X_calibrate, y_calibrate,\n        headlines_val=df_calibrate['headline'],\n        run_name=run_name,\n        hidden_dims=params['hidden_dims'],\n        lr=params['lr'],\n        dropout=params['dropout'],\n        epochs=15 # Reduced epochs for tuning\n    )\n    \n    tuning_results.append({**params, **metrics})\n    print(f\"Result: MSE={metrics['mse']:.6f}\")\n    \n    if metrics['mse'] < best_score:\n        best_score = metrics['mse']\n        best_params = params\n\nprint(\"\\nBest Parameters found:\", best_params)\npd.DataFrame(tuning_results).to_csv(TRAIN_DIR / \"tuning_results.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:29:40.690606Z","iopub.execute_input":"2025-11-23T18:29:40.690879Z","iopub.status.idle":"2025-11-23T18:29:57.500114Z","shell.execute_reply.started":"2025-11-23T18:29:40.690859Z","shell.execute_reply":"2025-11-23T18:29:57.499263Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Hyperparameter Tuning on Small Subset ---\n\nTesting params: {'hidden_dims': [512, 256], 'lr': 0.001, 'dropout': 0.2}\n[tune_run_0] Train shape: (10793, 1024), Val shape: (6072, 1024)\n[tune_run_0] Early stopping at epoch 13\nResult: MSE=0.000093\n\nTesting params: {'hidden_dims': [256, 128], 'lr': 0.001, 'dropout': 0.1}\n[tune_run_1] Train shape: (10793, 1024), Val shape: (6072, 1024)\n[tune_run_1] Early stopping at epoch 7\nResult: MSE=0.000094\n\nTesting params: {'hidden_dims': [512, 256], 'lr': 0.0005, 'dropout': 0.2}\n[tune_run_2] Train shape: (10793, 1024), Val shape: (6072, 1024)\n[tune_run_2] Early stopping at epoch 13\nResult: MSE=0.000096\n\nBest Parameters found: {'hidden_dims': [512, 256], 'lr': 0.001, 'dropout': 0.2}\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# --- C. Final Training (Full Data) ---\nprint(\"\\n--- Retraining Best Model on Full Training Data ---\")\n\nfinal_metrics, final_model_dir, x_scaler_final, y_scaler_final, best_model = train_mlp_specific_data(\n    X_train_full, y_train_full, \n    X_calibrate, y_calibrate,\n    headlines_val=df_calibrate['headline'],\n    run_name=\"final_best_model\",\n    hidden_dims=best_params['hidden_dims'],\n    lr=best_params['lr'],\n    dropout=best_params['dropout'],\n    epochs=50 # Full epochs\n)\n\nprint(f\"Final Validation Metrics: {final_metrics}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:30:13.500059Z","iopub.execute_input":"2025-11-23T18:30:13.500625Z","iopub.status.idle":"2025-11-23T18:30:40.555207Z","shell.execute_reply.started":"2025-11-23T18:30:13.500601Z","shell.execute_reply":"2025-11-23T18:30:40.554454Z"}},"outputs":[{"name":"stdout","text":"\n--- Retraining Best Model on Full Training Data ---\n[final_best_model] Train shape: (53965, 1024), Val shape: (6072, 1024)\n[final_best_model] Early stopping at epoch 13\nFinal Validation Metrics: {'mse': 7.794710857119753e-05, 'mae': 0.006212676950567204, 'r2': 0.31196912508675, 'best_val_loss_scaled': 0.5484905444477733}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"# --- D. Final Test Set Evaluation ---\nprint(\"\\n--- Evaluating on Test Set ---\")\n\nbest_model.eval()\nwith torch.no_grad():\n    # Transform test data using the scaler fitted on FULL training data\n    X_test_s = x_scaler_final.transform(X_test)\n    X_test_tensor = torch.from_numpy(X_test_s).float().to(DEVICE)\n    \n    # Predict\n    test_preds_s = best_model(X_test_tensor).cpu().numpy()\n    \n    # Inverse transform\n    test_preds_orig = y_scaler_final.inverse_transform(test_preds_s.reshape(-1, 1)).ravel()\n\n# Calculate Test Metrics\nmse_test = mean_squared_error(y_test, test_preds_orig)\nr2_test = r2_score(y_test, test_preds_orig)\n\nprint(f\"TEST SET RESULTS -> MSE: {mse_test:.6f}, R2: {r2_test:.4f}\")\n\n# Save Test Predictions\ntest_df_out = pd.DataFrame({\n    \"headline\": df_test['headline'],\n    \"CTR_true\": y_test,\n    \"CTR_pred\": test_preds_orig\n})\ntest_df_out.to_csv(TRAIN_DIR / \"final_test_predictions.csv\", index=False)\nprint(\"Saved final test predictions to\", TRAIN_DIR / \"final_test_predictions.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T18:30:49.788309Z","iopub.execute_input":"2025-11-23T18:30:49.789000Z","iopub.status.idle":"2025-11-23T18:30:49.955473Z","shell.execute_reply.started":"2025-11-23T18:30:49.788980Z","shell.execute_reply":"2025-11-23T18:30:49.954834Z"}},"outputs":[{"name":"stdout","text":"\n--- Evaluating on Test Set ---\nTEST SET RESULTS -> MSE: 0.000075, R2: 0.3273\nSaved final test predictions to training/final_test_predictions.csv\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}